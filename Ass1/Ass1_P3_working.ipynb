{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + (math.exp(-x)))\n",
    "\n",
    "def n_sigmoid(x):\n",
    "    return 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "def softmax(i, outputs):\n",
    "    denominator = sum(math.exp(o) for o in outputs)\n",
    "        \n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    \n",
    "    return math.exp(outputs[i]) / denominator\n",
    "\n",
    "def n_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Subtracting the max value for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    loss = 0\n",
    "    m = len(predictions)\n",
    "    \n",
    "    for j in range(m):\n",
    "        loss += targets[j] * math.log(predictions[j])\n",
    "    \n",
    "    return -1 * loss\n",
    "\n",
    "def binary_cross_entropy(prediction, target):\n",
    "    return -1 * (target * math.log(prediction) + (1 - target) * math.log(1 - prediction))\n",
    "\n",
    "def n_binary_cross_entropy(prediction, target):\n",
    "    return -np.sum(target * np.log(prediction) + (1 - target) * np.log(1 - prediction))\n",
    "\n",
    "def d_binary_cross_entropy(prediction, target):\n",
    "    return -(target / prediction) + ((1 - target) / (1 - prediction))\n",
    "\n",
    "def n_d_binary_cross_entropy(prediction, target):\n",
    "    return -(target / prediction) + ((1 - target) / (1 - prediction))\n",
    "\n",
    "def n_cross_entropy(predictions, targets):\n",
    "    return -np.sum(targets * np.log(predictions))\n",
    "\n",
    "def d_cross_entropy(predictions, targets):\n",
    "    return [predictions[i] - targets[i] for i in range(len(targets))]\n",
    "\n",
    "def n_d_cross_entropy(prediction, target):\n",
    "    return prediction - target\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "    \n",
    "def n_d_sigmoid(x):\n",
    "    s = n_sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def d_softmax_eq(i, outputs):\n",
    "    s = softmax(i, outputs)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def d_softmax_neq(i, j, outputs):\n",
    "    s_i = softmax(i, outputs)\n",
    "    s_j = softmax(j, outputs)\n",
    "    return -s_i * s_j\n",
    "            \n",
    "def n_d_softmax_eq(i, outputs):\n",
    "    s = n_softmax(i, outputs)\n",
    "    return s[i] * (1 - s[i])\n",
    "\n",
    "def n_d_softmax_neq(i, j, outputs):\n",
    "    s = n_softmax(i, outputs)\n",
    "    return -s[i] * s[j]            \n",
    "\n",
    "def softmax_derivative(x):\n",
    "    softmax_output = softmax(x)\n",
    "    n = len(x)\n",
    "    jacobian_matrix = []\n",
    "\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                row.append(softmax_output[i] * (1 - softmax_output[i]))\n",
    "            else:\n",
    "                row.append(-softmax_output[i] * softmax_output[j])\n",
    "        jacobian_matrix.append(row)\n",
    "\n",
    "    return jacobian_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size : 3\n",
    "# sigmoid : activation\n",
    "# in : in_act (x), weight (W), bias (b)\n",
    "# out : out_act = activ(in_act * weight + bias)\n",
    "\n",
    "# size : 2\n",
    "# softmax : activation\n",
    "# in : in_act (h), weight (V), bias (c)\n",
    "# out : out_act = activ(in_act * weight + bias)\n",
    "layers = [\n",
    "    {\n",
    "        'size': 3,\n",
    "        'activation': 'sigmoid',\n",
    "        'bias': True,\n",
    "        'weights': [[1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]],\n",
    "        'bias_weights': [0.0, 0.0, 0.0],\n",
    "    },\n",
    "    {\n",
    "        'size': 2,\n",
    "        'activation': 'softmax',\n",
    "        'bias': True,\n",
    "        'weights': [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]],\n",
    "        'bias_weights': [0.0, 0.0]\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_x = 2\n",
    "\n",
    "# initialize weights\n",
    "def initialize(layers):\n",
    "    in_act_size = num_of_x\n",
    "    for j in range(len(layers)):\n",
    "        s = layers[j]['size']\n",
    "        \n",
    "        if 'weights' not in layers[j]:\n",
    "            # populate weights\n",
    "            w = []\n",
    "            for _ in range(in_act_size): # for each input\n",
    "                w.append([random.uniform(0, 1) for _ in range(s)])\n",
    "            layers[j]['weights'] = w\n",
    "        \n",
    "        dw = []\n",
    "        for _ in range(in_act_size): # for each input\n",
    "            dw.append([0] * s)\n",
    "            \n",
    "        layers[j]['d_w'] = dw\n",
    "        \n",
    "        if 'bias_weights' not in layers[j]:\n",
    "            # populate bias\n",
    "            if layers[j]['bias']:\n",
    "                layers[j]['bias_weights'] = [random.uniform(0, 1) for _ in range(s)]\n",
    "                \n",
    "        layers[j]['d_b'] = [0] * s\n",
    "        layers[j]['d_k'] = [0] * s\n",
    "        layers[j]['d_h'] = [0] * s\n",
    "        layers[j]['d_x'] = [0] * in_act_size\n",
    "                \n",
    "            \n",
    "        in_act_size = s\n",
    "        \n",
    "initialize(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanze(layers):\n",
    "    for j in range(len(layers)):\n",
    "        layers[j]['d_k'] = [0] * len(layers[j]['d_k'])\n",
    "        layers[j]['d_h'] = [0] * len(layers[j]['d_h'])\n",
    "        layers[j]['d_x'] = [0] * len(layers[j]['d_x'])\n",
    "        layers[j]['in_act'] = [0] * len(layers[j]['in_act'])\n",
    "        layers[j]['pre_act'] = [0] * len(layers[j]['pre_act'])\n",
    "        layers[j]['post_act'] = [0] * len(layers[j]['post_act'])\n",
    "\n",
    "def cleanze_gradients(layers):\n",
    "    for j in range(len(layers)):\n",
    "        layers[j]['d_w'] = [[0.0 for _ in range(len(layers[j]['d_w'][0]))] for _ in range(len(layers[j]['d_w']))]\n",
    "        layers[j]['d_b'] = [0] * len(layers[j]['d_b'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(layers, in_act):\n",
    "    post_act = []\n",
    "    for j in range(len(layers)):\n",
    "        post_act.clear()\n",
    "        # for each layer\n",
    "        layer = layers[j]\n",
    "        layers[j]['in_act'] = in_act.copy()\n",
    "        \n",
    "        # post_act = activ(in_act * weight + bias)\n",
    "        weights = layer['weights']\n",
    "        num_of_neurons = layer['size']\n",
    "        act = layer['activation']\n",
    "        \n",
    "        pre_act = []\n",
    "        # for each neuron\n",
    "        for n in range(num_of_neurons):\n",
    "            # k\n",
    "            out_pre = 0\n",
    "            for i in range(len(in_act)):\n",
    "                out_pre += in_act[i] * weights[i][n] \n",
    "            if 'bias_weights' in layer:\n",
    "                out_pre += layer['bias_weights'][n]\n",
    "            pre_act.append(out_pre)\n",
    "            \n",
    "            # h\n",
    "            if act == 'sigmoid':\n",
    "                out_aft_act = sigmoid(out_pre)\n",
    "            post_act.append(out_aft_act)\n",
    "        layers[j]['pre_act'] = pre_act.copy()\n",
    "        \n",
    "        # o\n",
    "        if act == 'softmax':\n",
    "            out_softmax = []\n",
    "            for n in range(num_of_neurons):\n",
    "                out_softmax.append(softmax(n, post_act))\n",
    "            post_act = out_softmax.copy()\n",
    "        layers[j]['post_act'] = post_act.copy()\n",
    "        \n",
    "        # print(f'Layer: {j} - Output: {post_act}',)\n",
    "        \n",
    "        in_act = post_act.copy()\n",
    "        \n",
    "        \n",
    "    return post_act\n",
    "        \n",
    "\n",
    "forward()\n",
    "layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [1, 0]\n",
    "predictions = layers[-1]['post_act']\n",
    "loss = cross_entropy(predictions, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0] - targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = layers.copy()\n",
    "def n_forward():\n",
    "    in_act = np.array([1, -1]) # x inputs\n",
    "    \n",
    "    for j in range(len(n_layers)):\n",
    "        # for each layer\n",
    "        layer = n_layers[j]\n",
    "        n_layers[j]['in_act'] = in_act.copy()\n",
    "        \n",
    "        # post_act = activ(in_act * weight + bias)\n",
    "        weights = np.array(layer['weights'])\n",
    "        bias_weights = np.array(layer['bias_weights'])\n",
    "\n",
    "        pre_act = np.dot(in_act, weights)\n",
    "        \n",
    "        if 'bias_weights' in layer:\n",
    "            pre_act += bias_weights\n",
    "            \n",
    "        n_layers[j]['pre_act'] = pre_act.copy()\n",
    "        \n",
    "        act = layer['activation']\n",
    "        if act == 'sigmoid':\n",
    "            post_act = n_sigmoid(pre_act)\n",
    "        elif act == 'softmax':\n",
    "            post_act = n_softmax(pre_act)\n",
    "\n",
    "        n_layers[j]['post_act'] = post_act.copy()\n",
    "        \n",
    "        print(f'Layer: {j} - Output: {post_act}',)\n",
    "        \n",
    "        in_act = post_act.copy()\n",
    "        \n",
    "    return post_act\n",
    "\n",
    "n_forward()\n",
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [1, 0]\n",
    "predictions = n_layers[-1]['post_act']\n",
    "loss = n_cross_entropy(predictions, targets)\n",
    "print(loss)\n",
    "d_loss = d_cross_entropy(predictions, targets)\n",
    "print(d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward()\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size : 2\n",
    "# softmax : activation\n",
    "# in : out_act = activ(in_act * weight + bias)\n",
    "# out : in_act (h), weight (V), bias (c)\n",
    "\n",
    "# in : \n",
    "\n",
    "\n",
    "# size : 3\n",
    "# sigmoid : activation\n",
    "# in : in_act (x), weight (W), bias (b)\n",
    "# out : out_act = activ(in_act * weight + bias)\n",
    "\n",
    "# def backward():\n",
    "#     in_ = loss\n",
    "#     # dl/do\n",
    "#     d_h = [0.0, 0.0, 0.0] # d_cross_entropy(predictions, targets) # d_loss\n",
    "    \n",
    "#     for j in range(1): #len(layers)-1, 0, -1\n",
    "        \n",
    "#         layer = layers[j]\n",
    "#         num_of_neurons = layer['size']\n",
    "#         act = layer['activation']\n",
    "#         post_act = layer['post_act']\n",
    "#         in_act = layer['in_act']\n",
    "#         weights = layer['weights']\n",
    "        \n",
    "#         for n in range(num_of_neurons): \n",
    "#             # deriv post activ\n",
    "#             if act == 'sigmoid':\n",
    "#                 print(n, d_h, post_act)\n",
    "#                 layer['d_k'][n] = d_h[n] * post_act[n] * (1 - post_act[n])\n",
    "#             print(layer['d_k'])\n",
    "#             # deriv of weight\n",
    "#             for i in range(len(in_act)):\n",
    "#                 layer['d_w'][i][n] = layer['d_k'][n] * in_act[i]\n",
    "                \n",
    "#             # deriv of bias\n",
    "#             layer['d_b'] = layer['d_k'].copy()\n",
    "            \n",
    "#             # deriv of in\n",
    "#             for i in range(len(in_act)):\n",
    "#                 layer['d_x'][i] += layer['d_k'][n] * weights[i][n]\n",
    "                \n",
    "#             d_h = layer['d_x'].copy()\n",
    "            \n",
    "        \n",
    "# backward()\n",
    "# layers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = [math.exp(i) for i in x]\n",
    "    sum_e_x = sum(e_x)\n",
    "    softmax_output = [i / sum_e_x for i in e_x]\n",
    "    return softmax_output\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "x = [0.5, 1.5]\n",
    "softmax_derivative_result = softmax_derivative(x)\n",
    "softmax_derivative_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize(layers)\n",
    "post_act = forward(layers, [1, -1])\n",
    "print(post_act)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_act = forward(layers, [1, -1])\n",
    "print(post_act)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid and softmax gradient functions\n",
    "def sigmoid_gradient(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax_gradient(x):\n",
    "    sm = [0] * len(x)\n",
    "    max_x = max(x)\n",
    "    exp_x = [math.exp(xi - max_x) for xi in x]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    for i in range(len(x)):\n",
    "        sm[i] = exp_x[i] / sum_exp_x * (1 - exp_x[i] / sum_exp_x)\n",
    "    return sm\n",
    "\n",
    "def backward(layers, y_outputs, t_targets):\n",
    "    \n",
    "    d_h = layers[-1]['d_h']\n",
    "    for j in reversed(range(len(layers))):        \n",
    "        layer = layers[j]\n",
    "        act = layer['activation']\n",
    "        num_of_neurons = layer['size']\n",
    "        # pre_act = layer['pre_act']\n",
    "        # d_w = layer['d_w'] # d_v\n",
    "        # d_b = layer['d_b'] # d_c\n",
    "        # d_x = layer['d_x'] # d_h\n",
    "        weights = layer['weights']\n",
    "        \n",
    "        in_act = layer['in_act']\n",
    "        \n",
    "        if act == 'softmax':\n",
    "            d_k = [(y_outputs[n] - t_targets[n]) for n in range(num_of_neurons)] # d_o\n",
    "            layers[j]['d_k'] = d_k\n",
    "            \n",
    "            for n in range(num_of_neurons):\n",
    "                for i in range(len(in_act)):\n",
    "                    layers[j]['d_w'][i][n] += d_k[n] * in_act[i]\n",
    "                    layers[j]['d_x'][i] += d_k[n] *  weights[i][n]\n",
    "                \n",
    "            layers[j]['d_b'] = [layers[j]['d_b'][b] + d_k[b] for b in range(len(layers[j]['d_b']))]\n",
    "            \n",
    "        elif act == 'sigmoid':\n",
    "            post_act = layer['post_act']\n",
    "            d_k = [(d_h[n] * (post_act[n] * (1 - post_act[n]))) for n in range(num_of_neurons)]\n",
    "\n",
    "            for n in range(num_of_neurons):\n",
    "                for i in range(len(in_act)):\n",
    "                    layers[j]['d_w'][i][n] += d_k[n] * in_act[i]\n",
    "                    layers[j]['d_x'][i] += d_k[n] *  weights[i][n]\n",
    "                    \n",
    "            layers[j]['d_b'] = [layers[j]['d_b'][b] + d_k[b] for b in range(len(layers[j]['d_b']))]\n",
    "        \n",
    "        \n",
    "        d_h = layers[j]['d_x'].copy()\n",
    "        \n",
    "    return layers\n",
    "\n",
    "# Example usage:\n",
    "# import math\n",
    "\n",
    "# targets = [1, 0]\n",
    "# predictions = n_layers[-1]['post_act']\n",
    "# loss = cross_entropy(predictions, targets)\n",
    "# print(loss,predictions,targets)\n",
    "# d_loss = d_cross_entropy(predictions, targets)\n",
    "# print(d_loss)\n",
    "\n",
    "# loss_gradient = [-2.0, 2.0]  # Replace with the actual loss gradient\n",
    "\n",
    "# layers = backward(layers, [0.5, 0.5], [1, 0])\n",
    "# layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,1,1]\n",
    "y = [2,2,2]\n",
    "[x[i] + y[i] for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanze(layers)\n",
    "cleanze_gradients(layers)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_synth\n",
    "(xtrain, ytrain), (xval, yval), num_cls = load_synth()\n",
    "\n",
    "def normalize(arr, t_min, t_max):\n",
    "    norm_arr = []\n",
    "    diff = t_max - t_min\n",
    "    diff_arr = arr.max() - arr.min()    \n",
    "    for i in arr:\n",
    "        temp = (((i - arr.min())*diff)/diff_arr) + t_min\n",
    "        norm_arr.append(temp)\n",
    "    return norm_arr\n",
    "\n",
    "def target_arr(num_cls, t):\n",
    "    arr = [0] * num_cls\n",
    "    arr[t] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "norm_train_x = normalize(xtrain, 0, 1)\n",
    "# norm_val_x = normalize(xval, 0, 1)\n",
    "\n",
    "norm_train_y = normalize(ytrain, 0, 1)\n",
    "norm_val_y = normalize(yval, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: [{'size': 3, 'activation': 'sigmoid', 'bias': True, 'weights': [[0.06021192625193528, 0.0721780847622594, 0.14633454994537676], [0.49330473284942467, 0.0036015947349626876, 0.5505878879959706]], 'd_w': [[0, 0, 0], [0, 0, 0]], 'bias_weights': [0.9732960952105992, 0.5344317842077743, 0.7882559555133724], 'd_b': [0, 0, 0], 'd_k': [0, 0, 0], 'd_h': [0, 0, 0], 'd_x': [0, 0]}, {'size': 2, 'activation': 'softmax', 'bias': True, 'weights': [[0.33694857350342455, 0.8137090244733777], [0.6873574371937923, 0.6297137739921426], [0.4680289445511583, 0.48569777186806895]], 'd_w': [[0, 0], [0, 0], [0, 0]], 'bias_weights': [0.8124845178436907, 0.8588768277446455], 'd_b': [0, 0], 'd_k': [0, 0], 'd_h': [0, 0], 'd_x': [0, 0, 0]}]\n",
      "j: 0, OLD WEIGHTS: [[0.06021192625193528, 0.0721780847622594, 0.14633454994537676], [0.49330473284942467, 0.0036015947349626876, 0.5505878879959706]], OLD BIAS: [0.9732960952105992, 0.5344317842077743, 0.7882559555133724]\n",
      "j: 0, NEW WEIGHTS: [[0.04177079889868961, 0.07509667117797661, 0.14561496657362624], [0.4826708246780181, 0.00528457101621818, 0.5501729467968614]], NEW BIAS: [0.948218317916443, 0.5384007205284855, 0.7872774062524909]\n",
      "j: 1, OLD WEIGHTS: [[0.33694857350342455, 0.8137090244733777], [0.6873574371937923, 0.6297137739921426], [0.4680289445511583, 0.48569777186806895]], OLD BIAS: [0.8124845178436907, 0.8588768277446455]\n",
      "j: 1, NEW WEIGHTS: [[0.5689234981005487, 0.5817340998762536], [0.8803005936365409, 0.43677061754939395], [0.6947437265015924, 0.2589829899176348]], NEW BIAS: [1.1124845178436908, 0.5588768277446454]\n",
      "j: 0, OLD WEIGHTS: [[0.04177079889868961, 0.07509667117797661, 0.14561496657362624], [0.4826708246780181, 0.00528457101621818, 0.5501729467968614]], OLD BIAS: [0.948218317916443, 0.5384007205284855, 0.7872774062524909]\n",
      "j: 0, NEW WEIGHTS: [[0.04126362760573295, 0.0975091696157165, 0.16337710379567666], [0.4823783689700136, 0.01820853416196174, 0.5604153214917191]], NEW BIAS: [0.947528624250152, 0.5688790982144798, 0.8114318363472647]\n",
      "j: 1, OLD WEIGHTS: [[0.5689234981005487, 0.5817340998762536], [0.8803005936365409, 0.43677061754939395], [0.6947437265015924, 0.2589829899176348]], OLD BIAS: [1.1124845178436908, 0.5588768277446454]\n",
      "j: 1, NEW WEIGHTS: [[0.7986021166060125, 0.3520554813707897], [1.0737134710761111, 0.2433577401098236], [0.9213652230123834, 0.0323614934068438]], NEW BIAS: [1.412484517843691, 0.2588768277446454]\n",
      "j: 0, OLD WEIGHTS: [[0.04126362760573295, 0.0975091696157165, 0.16337710379567666], [0.4823783689700136, 0.01820853416196174, 0.5604153214917191]], OLD BIAS: [0.947528624250152, 0.5688790982144798, 0.8114318363472647]\n",
      "j: 0, NEW WEIGHTS: [[0.05895353656479974, 0.13881087679400334, 0.1988415796517977], [0.4925790938947117, 0.042024787531053126, 0.5808655885701739]], NEW BIAS: [0.9715848323513613, 0.6250445914972651, 0.8596593774481428]\n",
      "j: 1, OLD WEIGHTS: [[0.7986021166060125, 0.3520554813707897], [1.0737134710761111, 0.2433577401098236], [0.9213652230123834, 0.0323614934068438]], OLD BIAS: [1.412484517843691, 0.2588768277446454]\n",
      "j: 1, NEW WEIGHTS: [[1.0282168280168047, 0.12244076995999745], [1.2707019410501408, 0.046369270135794005], [1.1502658408834032, -0.19653912446417593]], NEW BIAS: [1.712484517843691, -0.04112317225535467]\n",
      "j: 0, OLD WEIGHTS: [[0.05895353656479974, 0.13881087679400334, 0.1988415796517977], [0.4925790938947117, 0.042024787531053126, 0.5808655885701739]], OLD BIAS: [0.9715848323513613, 0.6250445914972651, 0.8596593774481428]\n",
      "j: 0, NEW WEIGHTS: [[0.09404538405619295, 0.19777005323443403, 0.250211183199501], [0.5128144882196042, 0.07602305976899494, 0.6104874027067828]], NEW BIAS: [1.0193056424391862, 0.7052221786858607, 0.9295160327806808]\n",
      "j: 1, OLD WEIGHTS: [[1.0282168280168047, 0.12244076995999745], [1.2707019410501408, 0.046369270135794005], [1.1502658408834032, -0.19653912446417593]], OLD BIAS: [1.712484517843691, -0.04112317225535467]\n",
      "j: 1, NEW WEIGHTS: [[1.2600367553389131, -0.10937915736211098], [1.4741245035566342, -0.15705329237069945], [1.38356933320036, -0.4298426167811327]], NEW BIAS: [2.012484517843691, -0.3411231722553547]\n",
      "j: 0, OLD WEIGHTS: [[0.09404538405619295, 0.19777005323443403, 0.250211183199501], [0.5128144882196042, 0.07602305976899494, 0.6104874027067828]], OLD BIAS: [1.0193056424391862, 0.7052221786858607, 0.9295160327806808]\n",
      "j: 0, NEW WEIGHTS: [[0.14471689525843384, 0.2722504920669728, 0.3147540736514066], [0.5420337537617886, 0.11897152663543624, 0.6477054727865673]], NEW BIAS: [1.0882129737941328, 0.8065068681755614, 1.0172868182469286]\n",
      "j: 1, OLD WEIGHTS: [[1.2600367553389131, -0.10937915736211098], [1.4741245035566342, -0.15705329237069945], [1.38356933320036, -0.4298426167811327]], OLD BIAS: [2.012484517843691, -0.3411231722553547]\n",
      "j: 1, NEW WEIGHTS: [[1.4960853896361008, -0.3454277916592986], [1.6863507531741253, -0.3692795419881907], [1.622898653782873, -0.6691719373636456]], NEW BIAS: [2.312484517843691, -0.6411231722553548]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'size': 3,\n",
       "  'activation': 'sigmoid',\n",
       "  'bias': True,\n",
       "  'weights': [[0.14471689525843384, 0.2722504920669728, 0.3147540736514066],\n",
       "   [0.5420337537617886, 0.11897152663543624, 0.6477054727865673]],\n",
       "  'd_w': [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
       "  'bias_weights': [1.0882129737941328, 0.8065068681755614, 1.0172868182469286],\n",
       "  'd_b': [0, 0, 0],\n",
       "  'd_k': [0, 0, 0],\n",
       "  'd_h': [0, 0, 0],\n",
       "  'd_x': [0, 0],\n",
       "  'in_act': [0, 0],\n",
       "  'pre_act': [0, 0, 0],\n",
       "  'post_act': [0, 0, 0]},\n",
       " {'size': 2,\n",
       "  'activation': 'softmax',\n",
       "  'bias': True,\n",
       "  'weights': [[1.4960853896361008, -0.3454277916592986],\n",
       "   [1.6863507531741253, -0.3692795419881907],\n",
       "   [1.622898653782873, -0.6691719373636456]],\n",
       "  'd_w': [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\n",
       "  'bias_weights': [2.312484517843691, -0.6411231722553548],\n",
       "  'd_b': [0, 0],\n",
       "  'd_k': [0, 0],\n",
       "  'd_h': [0, 0],\n",
       "  'd_x': [0, 0, 0],\n",
       "  'in_act': [0, 0, 0],\n",
       "  'pre_act': [0, 0],\n",
       "  'post_act': [0, 0]}]"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layers = [\n",
    "    {\n",
    "        'size': 3,\n",
    "        'activation': 'sigmoid',\n",
    "        'bias': True,\n",
    "        # 'weights': [[1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]],\n",
    "        # 'bias_weights': [0.0, 0.0, 0.0],\n",
    "    },\n",
    "    {\n",
    "        'size': 2,\n",
    "        'activation': 'softmax',\n",
    "        'bias': True,\n",
    "        # 'weights': [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]],\n",
    "        # 'bias_weights': [0.0, 0.0]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 5\n",
    "\n",
    "# test_data = [\n",
    "#     [1, 0],\n",
    "#     [1, 0],\n",
    "#     [-1, 0],\n",
    "#     [-1, 0],\n",
    "#     [1, 0],\n",
    "#     [-1, 0],\n",
    "# ]\n",
    "\n",
    "# targets = [\n",
    "#     [1, 0],\n",
    "#     [1, 0],\n",
    "#     [0, 1],\n",
    "#     [0, 1],\n",
    "#     [1, 0],\n",
    "#     [0, 1],\n",
    "# ]\n",
    "\n",
    "ep_len = len(norm_train_x)\n",
    "\n",
    "\n",
    "initialize(layers)\n",
    "losses = []\n",
    "\n",
    "print(f'init: {layers}')\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for ep in range(ep_len):\n",
    "\n",
    "        # real\n",
    "        x1, x2 = norm_train_x[i]\n",
    "        y = 0 if norm_train_y[i] == 1 else 1 \n",
    "        t_targets = target_arr(num_cls, y)\n",
    "        x_inputs = [x1, x2]\n",
    "            \n",
    "        # test\n",
    "        # x_inputs = [1, -1]\n",
    "        # t_targets = [1, 0]\n",
    "            \n",
    "        # Forward Pass\n",
    "        post_act = forward(layers, x_inputs)\n",
    "        \n",
    "        # print(f'forward: {layers}')\n",
    "\n",
    "        # Calculate the cross-entropy loss\n",
    "        loss = cross_entropy(post_act, t_targets)\n",
    "\n",
    "        # Backward pass\n",
    "        backward(layers, post_act, t_targets)\n",
    "        \n",
    "        # print(f'backward: {layers}')\n",
    "\n",
    "        # Clear in (x), pre(k), post(h)\n",
    "        cleanze(layers)\n",
    "        \n",
    "        # if ep % 10 == 0:\n",
    "        #     print(f'Epoch {epoch + 1}/{epochs}, EP {ep+1}/{ep_len} Loss: {loss}, PA: {post_act}')\n",
    "        losses.append(loss)\n",
    "            \n",
    "    # Update weights and biases\n",
    "    for j in range(len(layers)):\n",
    "        print(f'j: {j}, OLD WEIGHTS: {layers[j]['weights']}, OLD BIAS: {layers[j]['bias_weights']}')\n",
    "        for n in range(layers[j]['size']):\n",
    "            for i in range(len(layers[j]['in_act'])):\n",
    "                layers[j]['weights'][i][n] += -learning_rate * layers[j]['d_w'][i][n]\n",
    "            if 'bias_weights' in layers[j]:\n",
    "                layers[j]['bias_weights'][n] += -learning_rate * layers[j]['d_b'][n]\n",
    "        print(f'j: {j}, NEW WEIGHTS: {layers[j]['weights']}, NEW BIAS: {layers[j]['bias_weights']}')\n",
    "                \n",
    "    cleanze_gradients(layers)\n",
    "    \n",
    "loss_train = losses\n",
    "episodes = range(0,len(losses))\n",
    "plt.plot(episodes, loss_train, 'g', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "      \n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_act = forward(layers, [50, 1, 1]) # [0, 1]\n",
    "post_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(target, output):\n",
    "    return -sum([target[i] * math.log(output[i]) for i in range(len(target))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'size': 3,\n",
       "  'activation': 'sigmoid',\n",
       "  'bias': True,\n",
       "  'weights': [[1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]],\n",
       "  'bias_weights': [0.0, 0.0, 0.0],\n",
       "  'd_w': [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
       "  'd_b': [0.0, 0.0, 0.0],\n",
       "  'd_k': [0, 0, 0],\n",
       "  'd_h': [0, 0, 0],\n",
       "  'd_x': [0.0, 0.0],\n",
       "  'in_act': [1, -1],\n",
       "  'pre_act': [2.0, 2.0, 2.0],\n",
       "  'post_act': [0.8807970779778823, 0.8807970779778823, 0.8807970779778823]},\n",
       " {'size': 2,\n",
       "  'activation': 'softmax',\n",
       "  'bias': True,\n",
       "  'weights': [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]],\n",
       "  'bias_weights': [0.0, 0.0],\n",
       "  'd_w': [[-0.44039853898894116, 0.44039853898894116],\n",
       "   [-0.44039853898894116, 0.44039853898894116],\n",
       "   [-0.44039853898894116, 0.44039853898894116]],\n",
       "  'd_b': [-0.5, 0.5],\n",
       "  'd_k': [-0.5, 0.5],\n",
       "  'd_h': [0, 0],\n",
       "  'd_x': [0.0, 0.0, 0.0],\n",
       "  'in_act': [0.8807970779778823, 0.8807970779778823, 0.8807970779778823],\n",
       "  'pre_act': [-0.8807970779778823, -0.8807970779778823],\n",
       "  'post_act': [0.5, 0.5]}]"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONE PASS\n",
    "layers = [\n",
    "    {\n",
    "        'size': 3,\n",
    "        'activation': 'sigmoid',\n",
    "        'bias': True,\n",
    "        'weights': [[1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]],\n",
    "        'bias_weights': [0.0, 0.0, 0.0],\n",
    "    },\n",
    "    {\n",
    "        'size': 2,\n",
    "        'activation': 'softmax',\n",
    "        'bias': True,\n",
    "        'weights': [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]],\n",
    "        'bias_weights': [0.0, 0.0]\n",
    "    },\n",
    "]\n",
    "\n",
    "epochs = 1\n",
    "ep_len = 1 \n",
    "\n",
    "initialize(layers)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for ep in range(1):\n",
    "            \n",
    "        # test\n",
    "        x_inputs = [1, -1]\n",
    "        t_targets = [1, 0]\n",
    "            \n",
    "        # Forward Pass\n",
    "        post_act = forward(layers, x_inputs)\n",
    "\n",
    "        # Calculate the cross-entropy loss\n",
    "        loss = cross_entropy(post_act, t_targets)\n",
    "\n",
    "        # Backward pass\n",
    "        backward(layers, post_act, t_targets)\n",
    "\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m n_d_sigmoid([\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32m/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb Cell 26\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb#X53sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mn_d_sigmoid\u001b[39m(x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb#X53sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     s \u001b[39m=\u001b[39m n_sigmoid(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb#X53sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m s \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m s)\n",
      "\u001b[1;32m/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb Cell 26\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mn_sigmoid\u001b[39m(x):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chrispickett/Desktop/DL/Ass1_P3.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m (np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39;49mx)))\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "n_d_sigmoid([0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
