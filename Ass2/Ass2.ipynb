{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Deep Learning - Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Working out backward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(x, y) = \\frac{x}{y} $. Then, $ f(x, y) $ can be rewritten as $ xy^{-1} $.\n",
    "The derivative with respect to x is:\n",
    "\n",
    "$$\n",
    "\\frac{df(x, y)}{dx} = y^{-1} = \\frac{1}{y}\n",
    "$$\n",
    "\n",
    "The derivative of a constant time a function is the constant times the derivative of the function. So applying the power rule, the derivative of x with respect to x is 1.\n",
    "Now, the derivative with respect to y is:\n",
    "\n",
    "$$\n",
    "\\frac{df(x, y)}{dy} = -xy^{-2} = -\\frac{x}{y^2}\n",
    "$$\n",
    "\n",
    "Again, by the power rule, the derivative of y with respect to y is -1 times $ y^{-2} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dimensions:\n",
    "- `f = 4` (features)\n",
    "- `m = 3` (output dimension)\n",
    "- `n = 2` (samples)\n",
    "\n",
    "**Forward pass:**\n",
    "- `X` is an `n x f` matrix with normally distributed random values. (`X.shape` is `(n, f)`)\n",
    "- `W` is an `m x f` matrix with normally distributed random values. (`W.shape` is `(m, f)`)\n",
    "- Compute `Y` as the dot product of `X` and the transpose of `W`, resulting in an `n x m` matrix. (`Y.shape` is `(n, m)`)\n",
    "\n",
    "**Copy of `Y` for gradient computation:**\n",
    "- `g` is a copy of `Y`.\n",
    "\n",
    "**Backward pass:**\n",
    "- Compute the gradient with respect to `W` as `dW = g.T @ X`, resulting in an `m x f` matrix. (`dW.shape` is `(m, f)`)\n",
    "- Compute the gradient with respect to `X` as `dx = g @ W`, resulting in an `n x f` matrix. (`dx.shape` is `(n, f)`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "(3, 4)\n",
      "(2, 3)\n",
      "(3, 4)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = 4; m = 3; n = 2;\n",
    "\n",
    "# forward\n",
    "X = np.random.randn(n, f); print(X.shape)\n",
    "W = np.random.randn(m, f); print(W.shape)\n",
    "Y = np.dot(X, W.T); print(Y.shape)\n",
    "\n",
    "g = Y.copy()\n",
    "\n",
    "# backward\n",
    "dW = np.matmul(g.T, X); print(dW.shape)\n",
    "dx = np.matmul(g, W); print(dx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(16, 1)\n",
      "(16, 5)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(5)\n",
    "num_columns = 16\n",
    "\n",
    "# forward\n",
    "x = x[None, :]; print(x.shape)\n",
    "y = np.ones([num_columns, 1]); print(y.shape)\n",
    "o = x * y; print(o.shape)\n",
    "\n",
    "# backward\n",
    "g = np.ones(o.shape[1]).squeeze(); print(g.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ./vugrad/experiments/train_mlp.py\n",
    "!python ./vugrad/experiments/train_mlp.py -D mnist -l 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.  3.1]]\n",
      "<vugrad.vugrad.core.OpNode object at 0x110e91b80>\n",
      "TensorNode[size (1, 2), source None].\n",
      "[[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import vugrad.vugrad as vg\n",
    "import numpy as np\n",
    "\n",
    "a = vg.TensorNode(np.array([[1.0, 1.1]]))\n",
    "b = vg.TensorNode(np.array([[2.0, 2.0]]))\n",
    "\n",
    "c = a + b;  print(c.value)\n",
    "\n",
    "# What does c.value contain?\n",
    "# In our newly created \"c\" TensorNode c.value contains the value of the operation add on the inputs a & b.\n",
    "# Resulting in a value of [3, 3.1] where a & b are [1.0, 1.1] & [2.0, 2.0] respectively\n",
    "\n",
    "# What does c.source refer to?\n",
    "# The source property refers to the operator node that created the output tensor node \"c\". \n",
    "# In our case the source refers to the Add operator node and is a reference to it.\n",
    "print(c.source)\n",
    "\n",
    "# What does c.source.inputs[0].value refer to?\n",
    "# The inputs of the source refers to the inputs of the operator node Add, which are our Tensor nodes a & b. \n",
    "# Thefore the 0th element of the sources input would be a since its the first input.\n",
    "print(c.source.inputs[0])\n",
    "\n",
    "# What does a.grad refer to? What is its current value?\n",
    "# It refers to the gradient of the tensor node \"a\" where the gradient will be stored when the loss is computed. \n",
    "# It is the deriviate of the loss with respect to param value\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) The Object is the Op class is the abstract class that represent this operation\n",
    "# 2) It's done within class Add(Op) on line 324, where \"return a + b\" occurs.\n",
    "# 3) The outputs are set to None because they have not been computed yet. ** \n",
    "# In do_forward in the Op class on line 249: opnode.outputs = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards is called in the Tensor Node which calls the source backwards : line 97 : self.source.backward()\n",
    "# Which calls the OpNode backward function that walks backwards down the graph to compute the gradients. \n",
    "# In tbe OpNode backward is called for the specific op : ginputs_raw = self.op.backward(self.context, *goutputs_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Op):\n",
    "    \"\"\"\n",
    "    Op for element-wise application of relu function\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(context, input):\n",
    "        relux = np.maximum(0, input)\n",
    "        context['relux'] = input\n",
    "        return relux\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, goutput):\n",
    "        relux = context['relux'] \n",
    "        return goutput * np.where(relux < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 60000 in training, 10000 in validation\n",
      " training class distribution: [32614 27386]\n",
      "     val. class distribution: [5513 4487]\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.4487\n",
      "   running loss: 0.2965\n",
      "epoch 001\n",
      "       accuracy: 0.9433\n",
      "   running loss: 0.1053\n",
      "epoch 002\n",
      "       accuracy: 0.9768\n",
      "   running loss: 0.06692\n",
      "epoch 003\n",
      "       accuracy: 0.9872\n",
      "   running loss: 0.05377\n",
      "epoch 004\n",
      "       accuracy: 0.9892\n",
      "   running loss: 0.04733\n",
      "epoch 005\n",
      "       accuracy: 0.9896\n",
      "   running loss: 0.04318\n",
      "epoch 006\n",
      "       accuracy: 0.9901\n",
      "   running loss: 0.04017\n",
      "epoch 007\n",
      "       accuracy: 0.991\n",
      "   running loss: 0.03785\n",
      "epoch 008\n",
      "       accuracy: 0.9921\n",
      "   running loss: 0.03597\n",
      "epoch 009\n",
      "       accuracy: 0.9924\n",
      "   running loss: 0.0344\n",
      "epoch 010\n",
      "       accuracy: 0.9931\n",
      "   running loss: 0.03305\n",
      "epoch 011\n",
      "       accuracy: 0.9934\n",
      "   running loss: 0.03188\n",
      "epoch 012\n",
      "       accuracy: 0.9934\n",
      "   running loss: 0.03084\n",
      "epoch 013\n",
      "       accuracy: 0.994\n",
      "   running loss: 0.02991\n",
      "epoch 014\n",
      "       accuracy: 0.9939\n",
      "   running loss: 0.02907\n",
      "epoch 015\n",
      "       accuracy: 0.9943\n",
      "   running loss: 0.0283\n",
      "epoch 016\n",
      "       accuracy: 0.9945\n",
      "   running loss: 0.02758\n",
      "epoch 017\n",
      "       accuracy: 0.9948\n",
      "   running loss: 0.02691\n",
      "epoch 018\n",
      "       accuracy: 0.995\n",
      "   running loss: 0.02623\n",
      "epoch 019\n",
      "       accuracy: 0.9953\n",
      "   running loss: 0.02552\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python ./vugrad/experiments/train_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 60000 in training, 10000 in validation\n",
      " training class distribution: [32511 27489]\n",
      "     val. class distribution: [5453 4547]\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.4491\n",
      "   running loss: 0.08491\n",
      "epoch 001\n",
      "       accuracy: 0.9789\n",
      "   running loss: 0.04412\n",
      "epoch 002\n",
      "       accuracy: 0.9803\n",
      "   running loss: 0.03986\n",
      "epoch 003\n",
      "       accuracy: 0.9822\n",
      "   running loss: 0.03802\n",
      "epoch 004\n",
      "       accuracy: 0.984\n",
      "   running loss: 0.03724\n",
      "epoch 005\n",
      "       accuracy: 0.9847\n",
      "   running loss: 0.0368\n",
      "epoch 006\n",
      "       accuracy: 0.9858\n",
      "   running loss: 0.03638\n",
      "epoch 007\n",
      "       accuracy: 0.9862\n",
      "   running loss: 0.03602\n",
      "epoch 008\n",
      "       accuracy: 0.987\n",
      "   running loss: 0.03583\n",
      "epoch 009\n",
      "       accuracy: 0.9877\n",
      "   running loss: 0.03563\n",
      "epoch 010\n",
      "       accuracy: 0.9888\n",
      "   running loss: 0.03525\n",
      "epoch 011\n",
      "       accuracy: 0.989\n",
      "   running loss: 0.03503\n",
      "epoch 012\n",
      "       accuracy: 0.9894\n",
      "   running loss: 0.03463\n",
      "epoch 013\n",
      "       accuracy: 0.9897\n",
      "   running loss: 0.03434\n",
      "epoch 014\n",
      "       accuracy: 0.9908\n",
      "   running loss: 0.03415\n",
      "epoch 015\n",
      "       accuracy: 0.9912\n",
      "   running loss: 0.03415\n",
      "epoch 016\n",
      "       accuracy: 0.9922\n",
      "   running loss: 0.03346\n",
      "epoch 017\n",
      "       accuracy: 0.992\n",
      "   running loss: 0.03332\n",
      "epoch 018\n",
      "       accuracy: 0.9926\n",
      "   running loss: 0.03369\n",
      "epoch 019\n",
      "       accuracy: 0.9927\n",
      "   running loss: 0.034\n"
     ]
    }
   ],
   "source": [
    "!python ./vugrad/experiments/train_mlp_relu.py\n",
    "# !python ./vugrad/experiments/train_mlp_relu.py -D mnist -l 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.0\n",
      "   running loss: 4.937e+18\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from _context import vugrad\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for running from the command line\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import vugrad.vugrad as vg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# (xtrain, ytrain), (xval, yval), num_classes = vg.load_synth()\n",
    "(xtrain, ytrain), (xval, yval), num_classes = vg.load_mnist(final=False, flatten=True)\n",
    "\n",
    "num_instances, num_features = xtrain.shape\n",
    "\n",
    "# Create a simple neural network.\n",
    "# This is a `Module` consisting of other modules representing linear layers, provided by the vugrad library.\n",
    "class MLP(vg.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP with one hidden layer, and a sigmoid non-linearity on the hidden layer and a softmax on the\n",
    "    output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_mult=4):\n",
    "        \"\"\"\n",
    "        :param input_size:\n",
    "        :param output_size:\n",
    "        :param hidden_mult: Multiplier that indicates how many times bigger the hidden layer is than the input layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = hidden_mult * input_size\n",
    "        # -- There is no common wisdom on how big the hidden size should be, apart from the idea\n",
    "        #    that it should be strictly _bigger_ than the input if at all possible.\n",
    "\n",
    "        self.layer1 = vg.Linear(input_size, hidden_size)\n",
    "        self.layer2 = vg.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = vg.Linear(hidden_size, hidden_size)\n",
    "        self.layer4 = vg.Linear(hidden_size, output_size)\n",
    "        # -- The linear layer (without activation) is implemented in vugrad. We simply instantiate these modules, and\n",
    "        #    add them to our network.\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        assert len(input.size()) == 2\n",
    "\n",
    "        # first layer\n",
    "        l1 = self.layer1(input)\n",
    "        h1 = vg.relu(l1)\n",
    "        \n",
    "        l2 = self.layer2(h1)\n",
    "        # h2 = vg.sigmoid(l2)\n",
    "        \n",
    "        # l3 = self.layer3(h2)\n",
    "        # h3 = vg.sigmoid(l3)\n",
    "\n",
    "        # l4 = self.layer4(h2)\n",
    "        \n",
    "        # softmax activation\n",
    "        output = vg.logsoftmax(l2)\n",
    "        # -- the logsoftmax computes the _logarithm_ of the probabilities produced by softmax. This makes the computation\n",
    "        #    of the CE loss more stable when the probabilities get close to 0 (remember that the CE loss is the logarithm\n",
    "        #    of these probabilities). It needs to be implemented in a specific way. See the source for details.\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "\n",
    "        return self.layer1.parameters() + self.layer2.parameters() #+ self.layer3.parameters() + self.layer4.parameters()\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "def learn():\n",
    "    ## Instantiate the model\n",
    "    mlp = MLP(input_size=num_features, output_size=num_classes)\n",
    "\n",
    "    n, m = xtrain.shape\n",
    "    b = 128\n",
    "    lr = 0.01\n",
    "    accurs = []\n",
    "\n",
    "    print('\\n## Starting training')\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'epoch {epoch:03}')\n",
    "\n",
    "        ## Compute validation accuracy\n",
    "        o = mlp(vg.TensorNode(xval))\n",
    "        oval = o.value\n",
    "\n",
    "        predictions = np.argmax(oval, axis=1)\n",
    "        num_correct = (predictions == yval).sum()\n",
    "        acc = num_correct / yval.shape[0]\n",
    "\n",
    "        o.clear() # gc the computation graph\n",
    "        print(f'       accuracy: {acc:.4}')\n",
    "\n",
    "        cl = 0.0 # running sum of the training loss\n",
    "\n",
    "        # We loop over the data in batches of size `b`\n",
    "        for fr in range(0, n, b):\n",
    "\n",
    "            # The end index of the batch\n",
    "            to = min(fr + b, n)\n",
    "\n",
    "            # Slice out the batch and its corresponding target values\n",
    "            batch, targets = xtrain[fr:to, :], ytrain[fr:to]\n",
    "\n",
    "            # Wrap the inputs in a Node\n",
    "            batch = vg.TensorNode(value=batch)\n",
    "\n",
    "            outputs = mlp(batch)\n",
    "            loss = vg.logceloss(outputs, targets)\n",
    "            # -- The computation graph is now complete. It consists of the MLP, together with the computation of\n",
    "            #    the scalar loss.\n",
    "            # -- The variable `loss` is the TensorNode at the very top of our computation graph. This means we can call\n",
    "            #    it to perform operations on the computation graph, like clearing the gradients, starting the backpropgation\n",
    "            #    and clearing the graph.\n",
    "            # -- Note that we set the MLP up to produce log probabilties, so we should compute the CE loss for these.\n",
    "\n",
    "            cl += loss.value\n",
    "            # -- We must be careful here to extract the _raw_ value for the running loss. What would happen if we kept\n",
    "            #    a running sum using the TensorNode?\n",
    "\n",
    "            # Start the backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # pply gradient descent\n",
    "            for parm in mlp.parameters():\n",
    "                parm.value -= lr * parm.grad\n",
    "                # -- Note that we are directly manipulating the members of the parm TensorNode. This means that for this\n",
    "                #    part, we are not building up a computation graph.\n",
    "\n",
    "            # -- In Pytorch, the gradient descent is abstracted away into an Optimizer. This allows us to build slightly more\n",
    "            #    complexoptimizers than plain graident descent.\n",
    "\n",
    "            # Finally, we need to reset the gradients to zero ...\n",
    "            loss.zero_grad()\n",
    "            # ... and delete the parts of the computation graph we don't need to remember.\n",
    "            loss.clear()\n",
    "\n",
    "        print(f'   running loss: {cl/n:.4}')\n",
    "        accurs.append(acc)\n",
    "    return accurs\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "xax = np.arange(epochs)\n",
    "y1 = learn()\n",
    "# y2 = learn()\n",
    "# y3 = learn()\n",
    "# y4 = learn()\n",
    "# y5 = learn()\n",
    "\n",
    "# mean = np.array([y1, y2, y3, y4, y5]).mean()\n",
    "mean = np.array([y1]).mean()\n",
    "print(mean)\n",
    "\n",
    "plt.plot(xax, y1, label='Line 1')\n",
    "# plt.plot(xax, y2, label='Line 2')\n",
    "# plt.plot(xax, y3, label='Line 3')\n",
    "# plt.plot(xax, y4, label='Line 4')\n",
    "# plt.plot(xax, y5, label='Line 5')\n",
    "\n",
    "# plt.plot(accurs)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.title('Test' )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar results but not as good. \n",
    "# Plot 10 trials each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,2,3], [2, 2, 1]])\n",
    "Y = np.array([[2,2,2], [3, 3, 3]])\n",
    "d_x = 1 / Y\n",
    "d_y = -X / (Y ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 55000 in training, 5000 in validation\n",
      " training class distribution: [5434 6212 5465 5622 5343 4963 5436 5702 5357 5466]\n",
      "     val. class distribution: [489 530 493 509 499 458 482 563 494 483]\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.095\n",
      "   running loss: 7.764\n",
      "epoch 001\n",
      "       accuracy: 0.0978\n",
      "   running loss: 7.747\n",
      "epoch 002\n",
      "       accuracy: 0.0978\n",
      "   running loss: 7.747\n",
      "epoch 003\n",
      "       accuracy: 0.0978\n",
      "   running loss: 7.747\n",
      "epoch 004\n",
      "       accuracy: 0.0978\n",
      "   running loss: 7.747\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python ./vugrad/experiments/train_mlp.py -D mnist -b 128 -e 5 -l 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 55000 in training, 5000 in validation\n",
      " training class distribution: [5434 6212 5465 5622 5343 4963 5436 5702 5357 5466]\n",
      "     val. class distribution: [489 530 493 509 499 458 482 563 494 483]\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "/Users/chrispickett/Desktop/DL/vugrad/vugrad/ops.py:181: RuntimeWarning: overflow encountered in exp\n",
      "  sigx =  1 / (1 + np.exp(-input))\n",
      "       accuracy: 0.0694\n",
      "   running loss: 8.521\n",
      "epoch 001\n",
      "       accuracy: 0.0964\n",
      "   running loss: 8.458\n",
      "epoch 002\n",
      "       accuracy: 0.0964\n",
      "   running loss: 8.451\n",
      "epoch 003\n",
      "       accuracy: 0.0964\n",
      "   running loss: 8.445\n",
      "epoch 004\n",
      "       accuracy: 0.0964\n",
      "   running loss: 8.439\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python ./vugrad/experiments/train_mlp.py -D mnist -b 128 -e 5 -l 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
